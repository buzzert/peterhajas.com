#!/bin/bash

GET_TIME="python -c 'import time; print time.time()'"
START_TIME=$(eval $GET_TIME)

# Check that we have mmd installed
METADATA_LINE_COUNT=5
WORKING_DIR=$(pwd)
HAS_MARKDOWN=$(which mmd | wc -l)
if [ $HAS_MARKDOWN -eq 0 ]; then
    echo "Please install mmd"
    exit -1
fi

# Create the output path
OUTPUT_PATH="out"
OUTPUT_INDEX=$OUTPUT_PATH/index.html
rm -r $OUTPUT_PATH
mkdir $OUTPUT_PATH

# We'll also use Markdown for the index
# We'll do this by keeping track of all the entries *by date* and then sorting
# them after we process them
touch dated_entries
touch temp

# Time to build the site!

MARKDOWN_START_TIME=$(eval $GET_TIME)

# Grab all the markdown files and make them into HTML
for markdownFile in $(find . -name '*.md' | sort); do
    FILENAME=$(basename $markdownFile)
    OUTPUTNAME=${FILENAME%.*}.html
    FULLPATH=$(dirname $markdownFile)
    RELATIVE_DIR=${FULLPATH/$WORKING_DIR/$OUTPUTPATH}
    RELATIVE_DIR=${RELATIVE_DIR/./}
    OUTPATH="$OUTPUT_PATH$RELATIVE_DIR/$OUTPUTNAME"
    TITLE=""
    PAGE_TITLE=${FILENAME%.*}
    # Uppercase page titles coming from filenames
    PAGE_TITLE="$(tr '[:lower:]' '[:upper:]' <<< ${PAGE_TITLE:0:1})${PAGE_TITLE:1}"
    PRETTY_DATE=""
    PATH_RELATIVE_TO_OUTPATH="$RELATIVE_DIR/$OUTPUTNAME"

    METADATA=$(cat $markdownFile | head -n $METADATA_LINE_COUNT)
    HAS_METADATA=$(echo "$METADATA" | head -n 2 | tail -n 1)
    if [ "$HAS_METADATA" = "<!--" ]; then
        TITLE=$(echo "$METADATA" | head -n 3 | tail -n 1)
        PAGE_TITLE=$TITLE
        DATE=$(echo "$METADATA" | tail -n 2 | head -n 1)
        # Date conversions in macOS: https://stackoverflow.com/a/43927574
        PRETTY_DATE=$(date -jf '%Y%m%d %H:%M' +'%B %e, %Y' "$DATE")
        echo "$DATE $markdownFile $PATH_RELATIVE_TO_OUTPATH $TITLE" >> dated_entries
    fi

    # Make directories until the outpath
    mkdir -p $OUTPATH
    # ...but delete the actual directory that was made for this filename
    rm -r $OUTPATH
    # Touch the file we'll output to
    touch $OUTPATH
    # Do the actual markdown generation
    cat $markdownFile | mmd > $OUTPATH

    # Do some post-processing
    # Insert before.html at the beginning
    cat before.html $OUTPATH > temp

    # Insert after.html at the end
    cat temp after.html > $OUTPATH

    # Insert title for the <title> tag
    sed -i '' -e "s/TITLE_FOR_PAGE_HERE/$PAGE_TITLE/g" $OUTPATH
    # Insert page title
    sed -i '' -e "s/TITLE_HERE/$TITLE/g" $OUTPATH
    # Insert date if necessary
    sed -i '' -e "s/DATE_HERE/$PRETTY_DATE/g" $OUTPATH
done

# sort dated_entries > dated_entries
sort --reverse dated_entries > temp
cp temp dated_entries

# Now process the dated entries into the index
touch the_index
echo -e "<ul id='index'>" >> the_index
while read datedEntry; do
    DATE=$(echo $datedEntry | awk '{ print $1 $2}')
    MARKDOWN_FILE=$(echo $datedEntry | awk '{ print $3 }')
    OUTLINK=$(echo $datedEntry | awk '{ print $4 }')
    TITLE=$(echo $datedEntry | cut -d " " -f 5-)

    PRETTY_DATE=$(date -jf '%Y%m%d%H:%M' +'%B %e, %Y' "$DATE")
    echo -e "<li class='indexItem'>\n" >> the_index
    echo -e "<h2 class='title'><a href='$OUTLINK'>$TITLE</a></h2>\n" >> the_index
    echo -e "<h3 class='date'>$PRETTY_DATE</h3>\n" >> the_index
    cat $MARKDOWN_FILE >> the_index
    echo -e "</li>\n" >> the_index
done < dated_entries
echo -e "</ul>" >> the_index

cat before.html > $OUTPUT_INDEX
cat the_index | mmd >> $OUTPUT_INDEX
cat after.html >> $OUTPUT_INDEX

# Insert a reasonable page title
sed -i '' -e "s/TITLE_FOR_PAGE_HERE/Index/g" $OUTPUT_INDEX
# Strip out title and date from index
sed -i '' -e "s/TITLE_HERE//g" $OUTPUT_INDEX
sed -i '' -e "s/DATE_HERE//g" $OUTPUT_INDEX

rm temp
rm dated_entries
rm the_index

MARKDOWN_END_TIME=$(eval $GET_TIME)
MARKDOWN_ELAPSED=$(echo "$MARKDOWN_END_TIME - $MARKDOWN_START_TIME" | bc)

echo "processed markdown in $MARKDOWN_ELAPSED"

MEDIA_START_TIME=$(eval $GET_TIME)

# Copy over css
cp style.css out/

# Copy over the media directory
cp -r media out/

# Perform post-processing on media
for MEDIA_FILE in $(find out/media); do
    MEDIA_INFO=$(file $MEDIA_FILE)
    IS_IMAGE=$(echo "$MEDIA_INFO" | grep "image data" | wc -l)
    FILENAME=$(basename $MEDIA_FILE)
    EXTENSION="${FILENAME##*.}"
    if (( $IS_IMAGE > 0 )); then
        # Let ffmpeg process the image to get it to be smaller
        TEMP_OUTPUT="temp.$EXTENSION"
        ffmpeg -i $MEDIA_FILE $TEMP_OUTPUT 1>/dev/null 2>/dev/null
        mv $TEMP_OUTPUT $MEDIA_FILE

        # Strip exif
        exiftool -overwrite_original -all= $MEDIA_FILE 1>/dev/null
    fi
done

MEDIA_END_TIME=$(eval $GET_TIME)
MEDIA_ELAPSED=$(echo "$MEDIA_END_TIME - $MEDIA_START_TIME" | bc)
echo "processed media in $MEDIA_ELAPSED"

# Finally, remove readme from output (we don't want included)
rm $OUTPUT_PATH/readme*

OUTPUT_SIZE=$(du -ck out | tail -n 1 | awk '{ print $1 }')

END_TIME=$(eval $GET_TIME)

ELAPSED=$(echo "$END_TIME - $START_TIME" | bc)
echo "built in $ELAPSED seconds"
echo "site is $OUTPUT_SIZE K"
